<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Statistics for Developers</title>
    <link rel="stylesheet" href="../css/styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <div class="home-image"><a href="../index.html"><img src="../images/chart.png" width="45" height="45" alt="Home"></a></div>
        <h1 class="logo">Theory Homework 8 </h1>
    </header>

    <main class="grid-layout">
        <!-- Left Sidebar with navigation menu -->
        <nav class="left-sidebar">
            <ul>
                <li><a href="../hw8/theoryhw8.html">Theory</a></li>
                <li><a href="../hw8/practicehw8.html">Practice</a></li>
            </ul>
        </nav>

        
        <!-- Main Content Area -->
        <section class="main-content">
            <h1>Shannon Entropy</h1>
            <p>
                Shannon entropy is a fundamental concept in information theory that measures the level of uncertainty or randomness in a probability distribution. 
                It quantifies the expected amount of information or "surprise" associated with a random variable. The mathematical definition is:
            </p>
            <p>
                \[
                H(X) = -\sum p(x) \log_b p(x)
                \]
            </p>
            <ul>
                <li><strong>X</strong>: A discrete random variable.</li>
                <li><strong>p(x)</strong>: The probability of each possible outcome \(x\).</li>
                <li><strong>b</strong>: The logarithm base, commonly set to 2 for measuring entropy in bits.</li>
            </ul>
            <p>
                Shannon entropy is widely applied in fields such as machine learning, cryptography, and data compression to assess diversity, randomness, or predictability within a system.
            </p>

            <h2>Alternative Measures of Diversity</h2>
            <p>In addition to Shannon entropy, several other metrics exist to quantify diversity or distribution spread:</p>
            <ul>
                <li><strong>Simpson's Index:</strong> Measures the probability that two randomly chosen elements belong to the same category, emphasizing dominance in a distribution.</li>
                <li><strong>Gini-Simpson Index:</strong> The complement of Simpson's Index, highlighting diversity rather than concentration.</li>
                <li><strong>RÃ©nyi Entropy:</strong> A generalization of Shannon entropy that introduces a tunable parameter, allowing emphasis on different characteristics of the distribution.</li>
            </ul>

            <h1>Primitive Root</h1>
            <p>
                A <strong>primitive root</strong> modulo a prime number \(p\) is an integer \(g\) such that every integer \(a\), which is coprime to \(p\), 
                can be expressed as \(g^k \mod p\) for some integer \(k\).
            </p>

            <h2>Definition</h2>
            <p>
                Formally, \(g\) is a primitive root modulo \(p\) if its powers generate all integers from 1 to \(p - 1\) under modulo \(p\). This means:
            </p>
            <ul>
                <li>The smallest positive integer \(k\) for which \(g^k \equiv 1 \mod p\) is exactly \(k = p - 1\).</li>
                <li>The powers of \(g\) produce distinct values modulo \(p\), ensuring that \(g^i \not\equiv g^j \mod p\) for \(0 \leq i < j \leq p - 1\).</li>
            </ul>
            <p>
                Primitive roots play a crucial role in number theory, particularly in cryptographic algorithms such as Diffie-Hellman key exchange and RSA encryption.
            </p>

        </section>

        <!-- Right Sidebar (empty for now) -->
        <section class="right-sidebar">

        </section>
        
    </main>
</body>
</html>
